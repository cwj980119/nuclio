# %% [markdown]
# # SAM (Segment Anything Model) 미세조정 및 평가

# %% [markdown]
# ## 필요한 라이브러리 임포트

# %%
import os
import json
import torch
import cv2
import numpy as np
from pycocotools import mask as coco_mask
from segment_anything import sam_model_registry, SamPredictor
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# %% [markdown]
# ## 커스텀 데이터셋 클래스 정의

# %%
class CustomDataset(Dataset):
    def __init__(self, annotation_path, image_dir, image_ids, mode='train'):
        # COCO 형식의 어노테이션 파일을 로드합니다.
        with open(annotation_path, 'r') as f:
            self.coco_data = json.load(f)
        self.image_dir = image_dir
        self.image_ids = image_ids
        # 이미지 ID와 파일 이름을 매핑합니다.
        self.id_to_filename = {img['id']: img['file_name'] for img in self.coco_data['images']}
        self.annotations = self.coco_data['annotations']
        self.mode = mode

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        # 주어진 인덱스에 해당하는 이미지와 어노테이션을 로드합니다.
        image_id = self.image_ids[idx]
        filename = self.id_to_filename[image_id]
        image_path = os.path.join(self.image_dir, filename)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        masks = []
        bboxes = []
        # 해당 이미지의 모든 어노테이션을 처리합니다.
        for ann in self.annotations:
            if ann['image_id'] == image_id:
                mask = coco_mask.decode(ann['segmentation'])
                masks.append(mask)
                bboxes.append(ann['bbox'])

        return image, masks, bboxes

# %% [markdown]
# ## 데이터셋 분할 함수 정의

# %%
def split_dataset(annotation_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    # COCO 어노테이션 파일을 로드합니다.
    with open(annotation_path, 'r') as f:
        coco_data = json.load(f)
    
    # 모든 이미지 ID를 추출합니다.
    image_ids = [img['id'] for img in coco_data['images']]
    
    # 데이터셋을 train, validation, test 세트로 분할합니다.
    train_ids, test_ids = train_test_split(image_ids, test_size=test_ratio, random_state=42)
    train_ids, val_ids = train_test_split(train_ids, test_size=val_ratio/(train_ratio+val_ratio), random_state=42)
    
    return train_ids, val_ids, test_ids

# %% [markdown]
# ## SAM 모델 학습 함수 정의

# %%
def train_sam(model, train_loader, val_loader, device, num_epochs=10, lr=1e-5):
    model.to(device)
    model.train()

    # 옵티마이저와 손실 함수를 정의합니다.
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = torch.nn.BCEWithLogitsLoss()

    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        # 훈련 데이터로더를 반복합니다.
        for image, masks, bboxes in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            # 이미지를 전처리하고 디바이스로 이동합니다.
            image = image.squeeze(0).permute(2, 0, 1).float().to(device)
            image = model.preprocess(image[None, :, :, :])
            
            # 이미지 임베딩을 계산합니다.
            with torch.no_grad():
                image_embedding = model.image_encoder(image)

            # 각 마스크와 바운딩 박스에 대해 처리합니다.
            for mask, bbox in zip(masks[0], bboxes[0]):
                mask_tensor = torch.from_numpy(mask).float().to(device)
                bbox_tensor = torch.tensor(bbox, dtype=torch.float, device=device)

                # 프롬프트 인코딩을 수행합니다.
                sparse_embeddings, dense_embeddings = model.prompt_encoder(
                    points=None,
                    boxes=bbox_tensor[None, :],
                    masks=None,
                )

                # 마스크를 예측합니다.
                low_res_masks, _ = model.mask_decoder(
                    image_embeddings=image_embedding,
                    image_pe=model.prompt_encoder.get_dense_pe(),
                    sparse_prompt_embeddings=sparse_embeddings,
                    dense_prompt_embeddings=dense_embeddings,
                    multimask_output=False,
                )

                # 손실을 계산하고 역전파를 수행합니다.
                loss = loss_fn(low_res_masks, mask_tensor[None, None, :, :])
                train_loss += loss.item()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        # 검증을 수행합니다.
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for image, masks, bboxes in tqdm(val_loader, desc="Validation"):
                # 검증 데이터에 대해 위와 동일한 과정을 수행합니다.
                image = image.squeeze(0).permute(2, 0, 1).float().to(device)
                image = model.preprocess(image[None, :, :, :])
                
                image_embedding = model.image_encoder(image)

                for mask, bbox in zip(masks[0], bboxes[0]):
                    mask_tensor = torch.from_numpy(mask).float().to(device)
                    bbox_tensor = torch.tensor(bbox, dtype=torch.float, device=device)

                    sparse_embeddings, dense_embeddings = model.prompt_encoder(
                        points=None,
                        boxes=bbox_tensor[None, :],
                        masks=None,
                    )

                    low_res_masks, _ = model.mask_decoder(
                        image_embeddings=image_embedding,
                        image_pe=model.prompt_encoder.get_dense_pe(),
                        sparse_prompt_embeddings=sparse_embeddings,
                        dense_prompt_embeddings=dense_embeddings,
                        multimask_output=False,
                    )

                    loss = loss_fn(low_res_masks, mask_tensor[None, None, :, :])
                    val_loss += loss.item()

        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}")

        # 최상의 모델을 저장합니다.
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), "best_sam_model.pth")

    print("Training completed.")

# %% [markdown]
# ## 마스크 시각화 함수 정의

# %%
def show_mask(mask, ax, random_color=False):
    # 마스크를 시각화하기 위한 색상을 설정합니다.
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

# %% [markdown]
# ## SAM 모델 평가 함수 정의

# %%
def evaluate_sam(model, test_loader, device):
    model.eval()
    predictor = SamPredictor(model)
    
    # 테스트 데이터로더를 반복합니다.
    for image, masks, bboxes in test_loader:
        image = image.squeeze(0).numpy()
        predictor.set_image(image)

        # 결과를 시각화합니다.
        plt.figure(figsize=(15, 5))
        plt.subplot(131)
        plt.imshow(image)
        plt.title("Original Image")
        plt.axis('off')

        plt.subplot(132)
        plt.imshow(image)
        for mask in masks[0]:
            show_mask(mask, plt.gca(), random_color=True)
        plt.title("Ground Truth")
        plt.axis('off')

        plt.subplot(133)
        plt.imshow(image)
        for bbox in bboxes[0]:
            # 각 바운딩 박스에 대해 마스크를 예측합니다.
            predicted_masks, _, _ = predictor.predict(
                point_coords=None,
                point_labels=None,
                box=bbox,
                multimask_output=False,
            )
            show_mask(predicted_masks[0], plt.gca(), random_color=True)
        plt.title("Predicted Masks")
        plt.axis('off')

        plt.show()

# %% [markdown]
# ## 메인 실행 코드

# %%
if __name__ == "__main__":
    # 경로를 설정합니다.
    annotation_path = "path/to/your/coco_annotations.json"
    image_dir = "path/to/your/image/directory"
    checkpoint = "path/to/sam_vit_h_4b8939.pth"

    # 데이터셋을 분할합니다.
    train_ids, val_ids, test_ids = split_dataset(annotation_path)

    # 데이터셋과 데이터로더를 생성합니다.
    train_dataset = CustomDataset(annotation_path, image_dir, train_ids, mode='train')
    val_dataset = CustomDataset(annotation_path, image_dir, val_ids, mode='val')
    test_dataset = CustomDataset(annotation_path, image_dir, test_ids, mode='test')

    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)

    # 디바이스를 설정하고 모델을 로드합니다.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = sam_model_registry["vit_h"](checkpoint=checkpoint)

    # 모델을 학습합니다.
    train_sam(model, train_dataloader, val_dataloader, device)

    # 최상의 모델을 로드합니다.
    model.load_state_dict(torch.load("best_sam_model.pth"))

    # 테스트 세트에 대해 모델을 평가합니다.
    evaluate_sam(model, test_dataloader, device)

# %% [markdown]
# 이 노트북을 실행하기 전에 다음 사항을 확인하세요:
# 1. 필요한 라이브러리가 모두 설치되어 있는지 확인합니다.
# 2. `annotation_path`, `image_dir`, `checkpoint` 변수를 귀하의 데이터셋과 모델 경로에 맞게 수정합니다.
# 3. 필요에 따라 학습 파라미터 (에폭 수, 학습률 등)를 조정합니다.
