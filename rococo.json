{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM (Segment Anything Model) 미세조정 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pycocotools import mask as coco_mask\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커스텀 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotation_path, image_dir, image_ids, mode='train'):\n",
    "        # COCO 형식의 어노테이션 파일을 로드합니다.\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_ids = image_ids\n",
    "        # 이미지 ID와 파일 이름을 매핑합니다.\n",
    "        self.id_to_filename = {img['id']: img['file_name'] for img in self.coco_data['images']}\n",
    "        self.annotations = self.coco_data['annotations']\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 해당하는 이미지와 어노테이션을 로드합니다.\n",
    "        image_id = self.image_ids[idx]\n",
    "        filename = self.id_to_filename[image_id]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        masks = []\n",
    "        bboxes = []\n",
    "        # 해당 이미지의 모든 어노테이션을 처리합니다.\n",
    "        for ann in self.annotations:\n",
    "            if ann['image_id'] == image_id:\n",
    "                mask = coco_mask.decode(ann['segmentation'])\n",
    "                masks.append(mask)\n",
    "                bboxes.append(ann['bbox'])\n",
    "\n",
    "        return image, masks, bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 분할 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(annotation_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    # COCO 어노테이션 파일을 로드합니다.\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # 모든 이미지 ID를 추출합니다.\n",
    "    image_ids = [img['id'] for img in coco_data['images']]\n",
    "    \n",
    "    # 데이터셋을 train, validation, test 세트로 분할합니다.\n",
    "    train_ids, test_ids = train_test_split(image_ids, test_size=test_ratio, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=val_ratio/(train_ratio+val_ratio), random_state=42)\n",
    "    \n",
    "    return train_ids, val_ids, test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM 모델 학습 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sam(model, train_loader, val_loader, device, num_epochs=10, lr=1e-5):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # 옵티마이저와 손실 함수를 정의합니다.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        # 훈련 데이터로더를 반복합니다.\n",
    "        for image, masks, bboxes in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # 이미지를 전처리하고 디바이스로 이동합니다.\n",
    "            image = image.squeeze(0).permute(2, 0, 1).float().to(device)\n",
    "            image = model.preprocess(image[None, :, :, :])\n",
    "            \n",
    "            # 이미지 임베딩을 계산합니다.\n",
    "            with torch.no_grad():\n",
    "                image_embedding = model.image_encoder(image)\n",
    "\n",
    "            # 각 마스크와 바운딩 박스에 대해 처리합니다.\n",
    "            for mask, bbox in zip(masks[0], bboxes[0]):\n",
    "                mask_tensor = torch.from_numpy(mask).float().to(device)\n",
    "                bbox_tensor = torch.tensor(bbox, dtype=torch.float, device=device)\n",
    "\n",
    "                # 프롬프트 인코딩을 수행합니다.\n",
    "                sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                    points=None,\n",
    "                    boxes=bbox_tensor[None, :],\n",
    "                    masks=None,\n",
    "                )\n",
    "\n",
    "                # 마스크를 예측합니다.\n",
    "                low_res_masks, _ = model.mask_decoder(\n",
    "                    image_embeddings=image_embedding,\n",
    "                    image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "                    sparse_prompt_embeddings=sparse_embeddings,\n",
    "                    dense_prompt_embeddings=dense_embeddings,\n",
    "                    multimask_output=False,\n",
    "                )\n",
    "\n",
    "                # 손실을 계산하고 역전파를 수행합니다.\n",
    "                loss = loss_fn(low_res_masks, mask_tensor[None, None, :, :])\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # 검증을 수행합니다.\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for image, masks, bboxes in tqdm(val_loader, desc=\"Validation\"):\n",
    "                # 검증 데이터에 대해 위와 동일한 과정을 수행합니다.\n",
    "                image = image.squeeze(0).permute(2, 0, 1).float().to(device)\n",
    "                image = model.preprocess(image[None, :, :, :])\n",
    "                \n",
    "                image_embedding = model.image_encoder(image)\n",
    "\n",
    "                for mask, bbox in zip(masks[0], bboxes[0]):\n",
    "                    mask_tensor = torch.from_numpy(mask).float().to(device)\n",
    "                    bbox_tensor = torch.tensor(bbox, dtype=torch.float, device=device)\n",
    "\n",
    "                    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                        points=None,\n",
    "                        boxes=bbox_tensor[None, :],\n",
    "                        masks=None,\n",
    "                    )\n",
    "\n",
    "                    low_res_masks, _ = model.mask_decoder(\n",
    "                        image_embeddings=image_embedding,\n",
    "                        image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "                        sparse_prompt_embeddings=sparse_embeddings,\n",
    "                        dense_prompt_embeddings=dense_embeddings,\n",
    "                        multimask_output=False,\n",
    "                    )\n",
    "\n",
    "                    loss = loss_fn(low_res_masks, mask_tensor[None, None, :, :])\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "        # 최상의 모델을 저장합니다.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_sam_model.pth\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마스크 시각화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    # 마스크를 시각화하기 위한 색상을 설정합니다.\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM 모델 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sam(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictor = SamPredictor(model)\n",
    "    \n",
    "    # 테스트 데이터로더를 반복합니다.\n",
    "    for image, masks, bboxes in test_loader:\n",
    "        image = image.squeeze(0).numpy()\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        # 결과를 시각화합니다.\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(image)\n",
    "        for mask in masks[0]:\n",
    "            show_mask(mask, plt.gca(), random_color=True)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(133
